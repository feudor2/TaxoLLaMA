{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f37e61-8916-4eb2-a5a2-2ca7c7308f59",
   "metadata": {},
   "source": [
    "# Collect samples for TaxoLLaMA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ab0187-a41d-44fe-bda7-70755c536728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T07:06:31.136898Z",
     "iopub.status.busy": "2025-04-27T07:06:31.135365Z",
     "iopub.status.idle": "2025-04-27T07:06:38.343505Z",
     "shell.execute_reply": "2025-04-27T07:06:38.342363Z",
     "shell.execute_reply.started": "2025-04-27T07:06:31.136840Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from typing import Union\n",
    "from random import sample\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from leafer import Leafer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3d2df-369f-4025-9536-1f2622ba7feb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create graph from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577a1d4c-b455-4e48-ad3b-d63e671fb760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:31:43.756984Z",
     "iopub.status.busy": "2025-04-27T00:31:43.756030Z",
     "iopub.status.idle": "2025-04-27T00:31:43.796843Z",
     "shell.execute_reply": "2025-04-27T00:31:43.795857Z",
     "shell.execute_reply.started": "2025-04-27T00:31:43.756938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_graph() -> nx.DiGraph:\n",
    "    '''\n",
    "    Create DAG from WordNet 3.0 synsets\n",
    "    \n",
    "    Returns: constructed graph\n",
    "    '''\n",
    "    G = nx.DiGraph()\n",
    "    for synset in tqdm(wn.all_synsets('n')):\n",
    "        name = synset.name()\n",
    "        G.add_node(name)\n",
    "        hyponyms = synset.hyponyms()\n",
    "\n",
    "        for hypo in hyponyms:\n",
    "            new_name = hypo.name()\n",
    "            G.add_node(new_name)\n",
    "            G.add_edge(name, new_name)\n",
    "\n",
    "    for synset in tqdm(wn.all_synsets('v')):\n",
    "        name = synset.name()\n",
    "        G.add_node(name)\n",
    "        hyponyms = synset.hyponyms()\n",
    "\n",
    "        for hypo in hyponyms:\n",
    "            new_name = hypo.name()\n",
    "            G.add_node(new_name)\n",
    "            G.add_edge(name, new_name)\n",
    "    return G\n",
    "\n",
    "def delete_cycles(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    '''\n",
    "    Delete cycles in DAG\n",
    "    \n",
    "    Returns: DAG without cycles\n",
    "    '''\n",
    "    while True:\n",
    "        try:\n",
    "            cycle = nx.find_cycle(G)\n",
    "            G.remove_edge(*cycle[0])\n",
    "        except:\n",
    "            break\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bdca1-01e8-4e90-9da2-1bf8ef82a967",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exclude test nodes from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52abe2c5-1ee6-4b6f-8e5f-21371aa5dc37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T07:14:21.658957Z",
     "iopub.status.busy": "2025-04-27T07:14:21.657775Z",
     "iopub.status.idle": "2025-04-27T07:14:21.679226Z",
     "shell.execute_reply": "2025-04-27T07:14:21.678142Z",
     "shell.execute_reply.started": "2025-04-27T07:14:21.658900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'MAGS': {\n",
    "        'cs': os.path.abspath(\"../datasets/TaxonomyEnrichment/data/MAG_CS/test_hypernyms_def.pickle\"),\n",
    "        'psy': os.path.abspath(\"../datasets/TaxonomyEnrichment/data/psychology/test_hypernyms_def.pickle\"),\n",
    "        'verb': os.path.abspath(\"../datasets/TaxonomyEnrichment/data/noun/test_hypernyms_def.pickle\"),\n",
    "        'noun': os.path.abspath(\"../datasets/TaxonomyEnrichment/data/verb/test_hypernyms_def.pickle\")\n",
    "    },\n",
    "    'SE': {\n",
    "        'main': os.path.abspath(\"../datasets/SemEval2018-Task9/custom_datasets/1A.english.pickle\"),\n",
    "        'medical': os.path.abspath(\"../datasets/SemEval2018-Task9/custom_datasets/2A.medical.pickle\"),\n",
    "        'music': os.path.abspath(\"../datasets/SemEval2018-Task9/custom_datasets/2B.music.pickle\")\n",
    "    },       \n",
    "    'TE': {\n",
    "        'env': os.path.abspath(\"../datasets/TExEval-2_testdata_1.2/all_data/gs_taxo/EN/environment_eurovoc_en.taxo\"),\n",
    "        'sci': os.path.abspath(\"../datasets/TExEval-2_testdata_1.2/all_data/gs_taxo/EN/science_eurovoc_en.taxo\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "822751e8-db19-43ed-8b5c-4ae1026274de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:01.045797Z",
     "iopub.status.busy": "2025-04-27T00:32:01.044415Z",
     "iopub.status.idle": "2025-04-27T00:32:01.071878Z",
     "shell.execute_reply": "2025-04-27T00:32:01.070959Z",
     "shell.execute_reply.started": "2025-04-27T00:32:01.045738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MAG\n",
    "def remove_mags(\n",
    "    G: nx.DiGraph,\n",
    "    paths: dict,\n",
    "    test_parents=[],\n",
    "    deleted=[],\n",
    "    k=0,\n",
    "    cs=True,\n",
    "    psy=True,\n",
    "    noun=True,\n",
    "    verb=True,\n",
    "    **kwargs\n",
    ") -> tuple[nx.DiGraph, list[str], list[str], int]:\n",
    "    '''\n",
    "    Associate items from MAG datasets with WordNet synsets and remove them from graph\n",
    "    \n",
    "    Arguments:\n",
    "        G - WordNet DAG\n",
    "        paths - dict with paths to datasets\n",
    "        test_parents - list of test nodes to fill, optional\n",
    "        deleted - list of deleted nodes to fill, optional\n",
    "        k - number of removed items, optional\n",
    "        cs - whether to remove MAG CS items, optional\n",
    "        psy - whether to remove MAG PSY items, optional\n",
    "        noun - whether to remove MAG Nouns items, optional\n",
    "        verb - whether to remove MAG Verbs items, optional\n",
    "        \n",
    "    Returns: G, test_parents, deleted, k\n",
    "    '''\n",
    "    if cs:\n",
    "        cs_test_path = paths['MAGS']['noun']\n",
    "\n",
    "        with open(cs_test_path, 'rb') as f:\n",
    "            cs_test = pickle.load(f)\n",
    "            \n",
    "        for node in cs_test:\n",
    "            for i in range(10):\n",
    "                true_name = f\"{node['children']}.n.0{i}\"\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "    if psy:\n",
    "        psy_test_path = paths['MAGS']['psy']\n",
    "\n",
    "        with open(psy_test_path, 'rb') as f:\n",
    "            psy_test = pickle.load(f)\n",
    "\n",
    "        for node in psy_test:\n",
    "            for i in range(10):\n",
    "                true_name = f\"{node['children']}.n.0{i}\"\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "    if verb:\n",
    "        verb_test_path = paths['MAGS']['verb']\n",
    "        \n",
    "    with open(verb_test_path, 'rb') as f:\n",
    "        verb_test = pickle.load(f)\n",
    "        \n",
    "    for node in verb_test:\n",
    "        if node['children'] in G.nodes():\n",
    "            test_parents.extend(G.predecessors(node['children']))\n",
    "            deleted.append(node['children'])\n",
    "            G.remove_node(node['children'])\n",
    "            k += 1\n",
    "    if noun:\n",
    "        noun_test_path = paths['MAGS']['noun']\n",
    "        \n",
    "        with open(noun_test_path, 'rb') as f:\n",
    "            noun_test = pickle.load(f)\n",
    "        \n",
    "        for node in noun_test:\n",
    "            if node['children'] in G.nodes():\n",
    "                test_parents.extend(G.predecessors(node['children']))\n",
    "                deleted.append(node['children'])\n",
    "                G.remove_node(node['children'])\n",
    "                k += 1\n",
    "    \n",
    "    return G, test_parents, deleted, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2f0af0-5b15-48a9-8c7d-fb231cd41ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:31:47.816816Z",
     "iopub.status.busy": "2025-04-27T00:31:47.815565Z",
     "iopub.status.idle": "2025-04-27T00:31:47.877932Z",
     "shell.execute_reply": "2025-04-27T00:31:47.877027Z",
     "shell.execute_reply.started": "2025-04-27T00:31:47.816748Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SemEval-2018\n",
    "def remove_semeval(\n",
    "    G: nx.DiGraph,\n",
    "    paths: dict,\n",
    "    test_parents=[],\n",
    "    deleted=[],\n",
    "    k=0,\n",
    "    main=True,\n",
    "    medical=False,\n",
    "    music=False,\n",
    "    **kwargs\n",
    ") -> tuple[nx.DiGraph, list[str], list[str], int]:\n",
    "    '''\n",
    "    Associate items from SemEval-2018 Task 9 datasets with WordNet synsets and remove them from graph\n",
    "        \n",
    "    Arguments:\n",
    "        G - WordNet DAG\n",
    "        paths - dict with paths to datasets\n",
    "        test_parents - list of test nodes to fill, optional\n",
    "        deleted - list of deleted nodes to fill, optional\n",
    "        k - number of removed items, optional\n",
    "        main - whether to remove SE-18 1A: English items, optional\n",
    "        medical - whether to remove SE-18 2A: Medical items, optional\n",
    "        music - whether to remove SE-18 2B: Music items, optional\n",
    "        \n",
    "    Returns: G, test_parents, deleted, k\n",
    "    '''\n",
    "    if main:\n",
    "        main_path = paths['SE']['main']\n",
    "\n",
    "        with open(main_path, 'rb') as f:\n",
    "            main = pickle.load(f)\n",
    "\n",
    "        for elem in main:\n",
    "            node = elem['children'].replace(' ', '_')\n",
    "            for i in range(10):\n",
    "                true_name = f'{node}.n.0{i}'\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "    if medical:\n",
    "        medical_path = paths['SE']['medical']\n",
    "        \n",
    "        with open(medical_path, 'rb') as f:\n",
    "            medical = pickle.load(f)\n",
    "            \n",
    "        for elem in medical:\n",
    "            node = elem['children'].replace(' ', '_')\n",
    "            for i in range(10):\n",
    "                true_name = f'{node}.n.0{i}'\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "    if music:\n",
    "        music_path = paths['SE']['music']\n",
    "\n",
    "        with open(music_path, 'rb') as f:\n",
    "            music = pickle.load(f)\n",
    "\n",
    "        for elem in music:\n",
    "            node = elem['children'].replace(' ', '_')\n",
    "            for i in range(10):\n",
    "                true_name = f'{node}.n.0{i}'\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "                \n",
    "    return G, test_parents, deleted, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7b3628-5e80-4b5f-8e7c-3c301ef36753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:31:49.919454Z",
     "iopub.status.busy": "2025-04-27T00:31:49.918221Z",
     "iopub.status.idle": "2025-04-27T00:31:49.965573Z",
     "shell.execute_reply": "2025-04-27T00:31:49.964592Z",
     "shell.execute_reply.started": "2025-04-27T00:31:49.919395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TexEval\n",
    "def remove_texeval(\n",
    "    G: nx.DiGraph,\n",
    "    paths: dict,\n",
    "    test_parents=[],\n",
    "    deleted=[],\n",
    "    k=0,\n",
    "    env=True,\n",
    "    sci=True,\n",
    "    **kwargs\n",
    ") -> tuple[nx.DiGraph, list[str], list[str], int]:\n",
    "    '''\n",
    "    Associate items from SemEval-2016 Task 13 (TexEval-2) datasets with WordNet synsets and remove them from graph\n",
    "        \n",
    "    Arguments:\n",
    "        G - WordNet DAG\n",
    "        paths - dict with paths to datasets\n",
    "        test_parents - list of test nodes to fill, optional\n",
    "        deleted - list of deleted nodes to fill, optional\n",
    "        k - number of removed items, optional\n",
    "        env - whether to remove SE-16 TaxEval Environment items, optional\n",
    "        sci - whether to remove SE-16 TaxEval Science items, optional\n",
    "        \n",
    "    Returns: G, test_parents, deleted, k\n",
    "    '''\n",
    "    \n",
    "    G_test = nx.DiGraph()\n",
    "    \n",
    "    if env:\n",
    "        env_path = paths['TE']['env']\n",
    "        \n",
    "        with open(env_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                idx, hypo, hyper = line.split(\"\\t\")\n",
    "                hyper = hyper.replace(\"\\n\", \"\")\n",
    "                G_test.add_node(hypo)\n",
    "                G_test.add_node(hyper)\n",
    "                G_test.add_edge(hyper, hypo)\n",
    "\n",
    "        for node in G_test.nodes():\n",
    "            for i in range(10):\n",
    "                true_name = f'{node}.n.0{i}'\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "    if sci:\n",
    "        sci_path = paths['TE']['sci']\n",
    "\n",
    "        with open(sci_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                idx, hypo, hyper = line.split(\"\\t\")\n",
    "                hyper = hyper.replace(\"\\n\", \"\")\n",
    "                G_test.add_node(hypo)\n",
    "                G_test.add_node(hyper)\n",
    "                G_test.add_edge(hyper, hypo)\n",
    "\n",
    "        for node in G_test.nodes():\n",
    "            for i in range(10):\n",
    "                true_name = f'{node}.n.0{i}'\n",
    "                if true_name in G.nodes():\n",
    "                    test_parents.extend(G.predecessors(true_name))\n",
    "                    deleted.append(true_name)\n",
    "                    G.remove_node(true_name)\n",
    "                    k += 1\n",
    "    return G, test_parents, deleted, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90908aa5-dfc9-4495-9a39-e55c2b45e3f3",
   "metadata": {},
   "source": [
    "### Create DAG from WordNet and clean it from test nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bdbc0b4-1a7d-428b-b5f2-a7352e650530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:07.004454Z",
     "iopub.status.busy": "2025-04-27T00:32:07.003206Z",
     "iopub.status.idle": "2025-04-27T00:32:07.044281Z",
     "shell.execute_reply": "2025-04-27T00:32:07.043318Z",
     "shell.execute_reply.started": "2025-04-27T00:32:07.004399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_cleaned_graph(paths:dict, MAGS=True, SE=True, TE=True, **kwargs) -> tuple[nx.DiGraph, list[str], list[str]]:\n",
    "    '''\n",
    "    Create DAG from WordNet 3.0 and remove test nodes from Taxonomy Enrichment and Hypernym Discovery datasets\n",
    "    Extract parent nodes of the test items\n",
    "    \n",
    "    Arguments:\n",
    "        paths - dict with paths to datasets\n",
    "        MAGS - whether to remove items from MAG datasets, optional\n",
    "        SE - whether to remove items from SemEval-2018 Task 9 datasets, optional\n",
    "        TE - whether to remove items from SemEval-2016 Task 13 datasets, optional\n",
    "        kwargs - dict to use to pass subtask dataset indicators, optional\n",
    "        \n",
    "    Returns:\n",
    "        G - created graph\n",
    "        test_parents - list of parents of the test items\n",
    "        deleted - list of removed test items\n",
    "    '''\n",
    "    G = build_graph()\n",
    "    k = 0\n",
    "    test_parents, deleted = [], []\n",
    "    if MAGS:\n",
    "        G, test_parents, deleted, k = remove_mags(G, paths, test_parents, deleted, k, **kwargs)\n",
    "    #print(k)\n",
    "    if SE:\n",
    "        G, test_parents, deleted, k = remove_semeval(G, paths, test_parents, deleted, k, **kwargs)\n",
    "    #print(k)\n",
    "    if TE:\n",
    "        G, test_parents, deleted, k = remove_texeval(G, paths, test_parents, deleted, k, **kwargs)\n",
    "    #print(k)\n",
    "    G = delete_cycles(G)\n",
    "    return G, test_parents, deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef5e30-62cb-4f1b-b811-62439d0b6490",
   "metadata": {},
   "source": [
    "## Tools to investigate and modify graph contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b742d8a0-6b63-4661-86a8-9b4139a29066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:08.823713Z",
     "iopub.status.busy": "2025-04-27T00:32:08.822548Z",
     "iopub.status.idle": "2025-04-27T00:32:08.845541Z",
     "shell.execute_reply": "2025-04-27T00:32:08.844701Z",
     "shell.execute_reply.started": "2025-04-27T00:32:08.823668Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sisters(G: nx.DiGraph, test_parents:list[str], ref_set=None) -> list[str]:\n",
    "    '''\n",
    "    Get children of the parent nodes \n",
    "    \n",
    "    Arguments:\n",
    "        G - WordNet graph, cleaned from test items\n",
    "        test_parents - parents of the removed test items\n",
    "        ref_set = list of items to check containment, optional\n",
    "        \n",
    "    Returns: list of children nodes from graph and ref_set if passed\n",
    "    '''\n",
    "    sisters = []\n",
    "    for n in set(test_parents):\n",
    "        try:\n",
    "            for sister in G.successors(n):\n",
    "                if not ref_set or sister in ref_set:\n",
    "                    sisters.append(sister)\n",
    "        except nx.NetworkXError:\n",
    "            #print(n)\n",
    "            pass\n",
    "    return list(set(sisters))\n",
    "\n",
    "\n",
    "def get_fraction(train_set: list[dict], sisters=None, out_list=False) -> Union[list[str], float]:\n",
    "    '''\n",
    "    Get items from the training sample:\n",
    "        get input items\n",
    "        get items from a specific set (sisters) -> only take items that occur as input hyponyms\n",
    "        get fraction of removed items from a specific set (sisters) if out_list is False\n",
    "    \n",
    "    Arguments:\n",
    "        train_set - list of dict with field 'children'\n",
    "        sisters - a set to search items from\n",
    "        out_list = get fraction of items from sisters, that are NOT present in the train_set\n",
    "        \n",
    "    Returns: list of str or float\n",
    "    '''\n",
    "    if sisters is not None:\n",
    "        rest = list(set(\n",
    "            [elem['children'] for elem in train_set if elem['children'] in sisters]\n",
    "        ))\n",
    "        return rest if out_list else 1 - len(rest)/len(sisters)\n",
    "    return [elem['children'] for elem in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e57953ea-36d1-46c9-8de4-c363e294f3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:10.987194Z",
     "iopub.status.busy": "2025-04-27T00:32:10.985970Z",
     "iopub.status.idle": "2025-04-27T00:32:11.002947Z",
     "shell.execute_reply": "2025-04-27T00:32:11.001895Z",
     "shell.execute_reply.started": "2025-04-27T00:32:10.987137Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_sample(sample: list[dict], mode: str, path: str) -> None:\n",
    "    '''\n",
    "    Pickle a training sample\n",
    "    \n",
    "    Arguments\n",
    "        sample - training sample\n",
    "        mode - experimental condition (for printing)\n",
    "        path - saving path\n",
    "    '''\n",
    "    print(mode, len(sample))\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(sample, f)\n",
    "        \n",
    "def get_train(G: nx.DiGraph, seed=42, with_test=False) -> Unioin[tuple[list[dict], list[dict]], list[dict]]:\n",
    "    '''\n",
    "    Sample hyponym-hypernym pairs from WordNet DAG\n",
    "    \n",
    "    Arguments\n",
    "        G - WordNet DAG\n",
    "        seed - random seed, optional\n",
    "        with_test - divide into train and test, optional\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    l = Leafer(G)\n",
    "    p = 0.001 if with_test else 0.0\n",
    "    train, test = l.split_train_test(\n",
    "        generation_depth=0,\n",
    "        p=p,\n",
    "        p_divide_leafs=0.5,\n",
    "        min_to_test_rate=0.5,\n",
    "        weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.],\n",
    "    )\n",
    "    return (train, test) if with_test else train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3cc5f58-8864-4f91-8d2e-3eb7c8e7ad75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:12.741913Z",
     "iopub.status.busy": "2025-04-27T00:32:12.740613Z",
     "iopub.status.idle": "2025-04-27T00:32:12.757558Z",
     "shell.execute_reply": "2025-04-27T00:32:12.756597Z",
     "shell.execute_reply.started": "2025-04-27T00:32:12.741858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base_sample(path, **kwargs) -> None:\n",
    "    '''\n",
    "    Get best sample from graph (only test node deleted)\n",
    "    '''\n",
    "    G, test_parents, _ = create_cleaned_graph(**kwargs)\n",
    "    train = get_train(G)\n",
    "    save_sample(train, 'full', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4498824-a420-425b-9f03-519f8bfdc3ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:14.636849Z",
     "iopub.status.busy": "2025-04-27T00:32:14.635589Z",
     "iopub.status.idle": "2025-04-27T00:32:14.649752Z",
     "shell.execute_reply": "2025-04-27T00:32:14.648816Z",
     "shell.execute_reply.started": "2025-04-27T00:32:14.636797Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SE181A = dict(\n",
    "    paths = DATASET_PATHS,\n",
    "    TE=False,\n",
    "    MAGS=False,\n",
    "    main=True,\n",
    "    medical=False,\n",
    "    music=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688cc552-2be6-4b96-97dc-45ae75057d0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T07:18:07.114799Z",
     "iopub.status.busy": "2025-04-27T07:18:07.113230Z",
     "iopub.status.idle": "2025-04-27T07:18:07.141214Z",
     "shell.execute_reply": "2025-04-27T07:18:07.140109Z",
     "shell.execute_reply.started": "2025-04-27T07:18:07.114736Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIRNAME = os.path.abspath(\"../samples\")\n",
    "if not os.path.isdir(DIRNAME):\n",
    "    os.mkdir(DIRNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c545cdcc-395a-4e27-936a-10f66a1dc928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:17:23.639878Z",
     "iopub.status.busy": "2025-04-26T19:17:23.638774Z",
     "iopub.status.idle": "2025-04-26T19:17:29.926158Z",
     "shell.execute_reply": "2025-04-26T19:17:29.925162Z",
     "shell.execute_reply.started": "2025-04-26T19:17:23.639808Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 40636 40636\n",
      "full 40636\n"
     ]
    }
   ],
   "source": [
    "#construct full dataset\n",
    "get_base_sample(os.path.join(DIRNAME, 'clean_train.pickle'), **SE181A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "907ec1c3-2e96-47f7-9926-a0e77b284938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:19.795922Z",
     "iopub.status.busy": "2025-04-27T00:32:19.794736Z",
     "iopub.status.idle": "2025-04-27T00:32:19.814808Z",
     "shell.execute_reply": "2025-04-27T00:32:19.813810Z",
     "shell.execute_reply.started": "2025-04-27T00:32:19.795880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c16a4-13d4-4994-b314-e74d827c18de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constructing samples with removed items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbd4ea-59c2-4b5e-a39b-d349564566bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Removing cohyponym items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e385c306-348e-4c90-b9a7-e7af415f1e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:21.776512Z",
     "iopub.status.busy": "2025-04-27T00:32:21.774487Z",
     "iopub.status.idle": "2025-04-27T00:32:45.974399Z",
     "shell.execute_reply": "2025-04-27T00:32:45.973353Z",
     "shell.execute_reply.started": "2025-04-27T00:32:21.776453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82115it [00:05, 14036.85it/s]\n",
      "13767it [00:00, 16165.52it/s]\n"
     ]
    }
   ],
   "source": [
    "G, test_parents, _ = create_cleaned_graph(**SE181A)\n",
    "sisters = get_sisters(G, test_parents)\n",
    "cleaned_full = read_file(os.path.join(DIRNAME, 'clean_train.pickle'))\n",
    "rest_sisters = get_fraction(cleaned_full, sisters, out_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5039976-1ba0-4f6f-b9ef-61e1d807cbe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:45.976602Z",
     "iopub.status.busy": "2025-04-27T00:32:45.975962Z",
     "iopub.status.idle": "2025-04-27T00:32:45.999180Z",
     "shell.execute_reply": "2025-04-27T00:32:45.998267Z",
     "shell.execute_reply.started": "2025-04-27T00:32:45.976562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pair_for_sisters(sisters: list[str], dataset: list[dict]) -> dict:\n",
    "    '''\n",
    "    Get (hyponym, hypernym) pairs for sisters in dataset where sisters are either children or parents\n",
    "    \n",
    "    Arguments:\n",
    "      sisters - list of sister node names\n",
    "      dataset - trainig dataset to pick sisters from\n",
    "      \n",
    "    Return: dict with sisters as children and as parents\n",
    "    '''\n",
    "    return {\n",
    "        'children': [(item['children'], item['parents']) for item in tqdm(dataset) if item['children'] in sisters], #if any([sister in item['children'] for sister in sisters])],\n",
    "        'parents': [(item['children'], item['parents']) for item in tqdm(dataset) if item['parents'] in sisters]#if any([sister in item['parents'] for sister in sisters])],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bdaec0f-04a7-4906-b9f1-3f96186a9b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:46.166494Z",
     "iopub.status.busy": "2025-04-27T00:32:46.165182Z",
     "iopub.status.idle": "2025-04-27T00:32:52.247509Z",
     "shell.execute_reply": "2025-04-27T00:32:52.246416Z",
     "shell.execute_reply.started": "2025-04-27T00:32:46.166434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40636/40636 [00:02<00:00, 13588.27it/s]\n",
      "100%|██████████| 40636/40636 [00:03<00:00, 13331.31it/s]\n"
     ]
    }
   ],
   "source": [
    "sister_pairs = pair_for_sisters(rest_sisters, cleaned_full)\n",
    "total_sister_items = sorted(list(set(sister_pairs['children'] + sister_pairs['parents'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38de5049-13cc-4c05-af63-b883a7de8766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:52.250751Z",
     "iopub.status.busy": "2025-04-27T00:32:52.249351Z",
     "iopub.status.idle": "2025-04-27T00:32:52.278539Z",
     "shell.execute_reply": "2025-04-27T00:32:52.277492Z",
     "shell.execute_reply.started": "2025-04-27T00:32:52.250691Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8776"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_sister_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3747ae6-cb98-402c-849c-91f58a7b4399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:52.281096Z",
     "iopub.status.busy": "2025-04-27T00:32:52.280361Z",
     "iopub.status.idle": "2025-04-27T00:32:52.316860Z",
     "shell.execute_reply": "2025-04-27T00:32:52.315620Z",
     "shell.execute_reply.started": "2025-04-27T00:32:52.281041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATSET SIZE:\t\t\t 40636\n",
      "TOTAL NUMBER OF SISTERS:\t 15295\n",
      "FINAL NUMBER OF SISTERS:\t 5535\n"
     ]
    }
   ],
   "source": [
    "print('DATSET SIZE:\\t\\t\\t', len(cleaned_full))\n",
    "print('TOTAL NUMBER OF SISTERS:\\t', len(sisters))\n",
    "print('FINAL NUMBER OF SISTERS:\\t', len(rest_sisters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c125a8fd-0dcc-4370-8330-68dffcec5e24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:52.703987Z",
     "iopub.status.busy": "2025-04-27T00:32:52.702680Z",
     "iopub.status.idle": "2025-04-27T00:32:52.737946Z",
     "shell.execute_reply": "2025-04-27T00:32:52.736756Z",
     "shell.execute_reply.started": "2025-04-27T00:32:52.703928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sister_to_items(sisters: list[str], pairs: dict):\n",
    "    '''\n",
    "    Refactor `pair_for_sisters` output to {synset_name: {children: [], parents: []}, ...}\n",
    "    \n",
    "    Arguments:\n",
    "      sisters - list of sister synset names\n",
    "      pairs - dict of format {children: [], parents: []}\n",
    "      \n",
    "    Return: dict with sisters as children and as parents\n",
    "    '''\n",
    "    return {\n",
    "        sister: {\n",
    "            'children': [item for item in pairs['children'] if sister == item[0]],\n",
    "            'parents': [item for item in pairs['parents'] if sister == item[1]],\n",
    "        } for sister in sisters\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85070c2c-5672-47b9-927b-de9f52ab96a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:32:56.130100Z",
     "iopub.status.busy": "2025-04-27T00:32:56.128783Z",
     "iopub.status.idle": "2025-04-27T00:32:59.066103Z",
     "shell.execute_reply": "2025-04-27T00:32:59.064997Z",
     "shell.execute_reply.started": "2025-04-27T00:32:56.130056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sisters_with_items = sister_to_items(rest_sisters, sister_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6db102e-d529-4d3c-a76f-9f103c4053a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:33:01.106520Z",
     "iopub.status.busy": "2025-04-27T00:33:01.104882Z",
     "iopub.status.idle": "2025-04-27T00:33:01.130670Z",
     "shell.execute_reply": "2025-04-27T00:33:01.129669Z",
     "shell.execute_reply.started": "2025-04-27T00:33:01.106461Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 5329, 2: 202, 3: 4})\n",
      "Counter({0: 4319, 1: 534, 2: 275, 3: 129, 4: 61, 5: 59, 6: 42, 7: 19, 9: 17, 8: 15, 11: 12, 10: 11, 12: 9, 13: 6, 15: 4, 14: 4, 22: 2, 19: 2, 21: 2, 23: 2, 18: 2, 17: 2, 26: 1, 39: 1, 42: 1, 27: 1, 20: 1, 24: 1, 16: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter([len(sisters_with_items[k]['children']) for k in sisters_with_items])\n",
    "print(cnt)\n",
    "cnt = Counter([len(sisters_with_items[k]['parents']) for k in sisters_with_items])\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c3d89be-9641-4835-8b76-0b94bbdb8057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:33:03.231930Z",
     "iopub.status.busy": "2025-04-27T00:33:03.230543Z",
     "iopub.status.idle": "2025-04-27T00:33:03.291012Z",
     "shell.execute_reply": "2025-04-27T00:33:03.289901Z",
     "shell.execute_reply.started": "2025-04-27T00:33:03.231879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_sisters(sisters: dict):\n",
    "    '''\n",
    "    Sort sisters by number of associated items\n",
    "    '''\n",
    "    sisters = {k: v['parents'] + v['children'] for k, v in sisters.items()}\n",
    "    return sorted(list(sisters.items()), key=lambda x: len(x[1])) #stack\n",
    "\n",
    "def get_shards(sisters: list[dict], total_sister_items: list[tuple]) -> list[list]:\n",
    "    '''\n",
    "    Prepare shards for 4-fold cross-validation: normalize by number of items associated to each sister in shard\n",
    "    \n",
    "    Arguments:\n",
    "        sisters - sister with associated items terms sorted by number of associated items\n",
    "        total_sisters - list of all items associated with sisters\n",
    "        \n",
    "    Returns: list of shards\n",
    "    '''\n",
    "    shard_length = len(total_sister_items) // 4 #4 fold cross-validation\n",
    "    shards = [[], [], [], []]\n",
    "    added = []\n",
    "    while any([len(shard) < shard_length for shard in shards]) and sisters: #stack is not empty\n",
    "        for i in range(4):\n",
    "            if len(shards[i]) >= shard_length:\n",
    "                continue\n",
    "            _, v = sisters.pop() #taking the sister with maximum number of associated items from the stack\n",
    "            to_add = [item for item in v if item not in added]\n",
    "            added.extend(to_add)\n",
    "            shards[i].extend(to_add)\n",
    "            continue\n",
    "    return shards\n",
    "            \n",
    "\n",
    "def get_sample(shards: list[list], dataset: list[dict], scope: float, i: int) -> list[dict]:\n",
    "    '''\n",
    "    Get sample for 4-fold cross-validation depending on proportion and index of fold\n",
    "    \n",
    "    Arguments:\n",
    "        shards - data partitions to use in sample construction\n",
    "        dataset - full training sample\n",
    "        scope - proportion of removed items\n",
    "        i - index of shard\n",
    "        \n",
    "    Returns: list of items dicts\n",
    "    '''\n",
    "    if scope == 0.25:\n",
    "        to_remove = shards[i]\n",
    "    if scope == 0.75:\n",
    "        to_remove = [pair for j in range(4) for pair in shards[j] if j != i]\n",
    "    if scope == 0.5:\n",
    "        indexes = list(combinations(range(4), 2))[i]\n",
    "        to_remove = shards[indexes[0]] + shards[indexes[1]]\n",
    "    if scope == 1:\n",
    "        to_remove = [item for shard in shards for item in shard]\n",
    "    #print(scope, i, to_remove, sep='\\n')\n",
    "    return [item for item in dataset if (item['children'], item['parents']) not in to_remove]\n",
    "        \n",
    "    \n",
    "def get_sister_folds(shards: list[dict], dataset: list[dict], scope: float) -> list[dict]:\n",
    "    '''\n",
    "    Generate samples by removing shards from the training set following 4-fold cross-validation algorithm\n",
    "    \n",
    "    Arguments:\n",
    "        shards - data partitions to use in sample construction\n",
    "        dataset - trainig sample\n",
    "        scope - proportion of items to remove\n",
    "        \n",
    "    Returns: list of concstructed samples\n",
    "    '''\n",
    "    k = 6 if scope == 0.5 else 4\n",
    "    return [get_sample(shards, dataset, scope, i) for i in tqdm(range(k))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d25313c-99e0-4ad2-ab51-e5a744e46d38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:33:54.959193Z",
     "iopub.status.busy": "2025-04-27T00:33:54.957877Z",
     "iopub.status.idle": "2025-04-27T00:35:21.411835Z",
     "shell.execute_reply": "2025-04-27T00:35:21.410961Z",
     "shell.execute_reply.started": "2025-04-27T00:33:54.959143Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 38442\n",
      "sister 38442\n",
      "sister 38442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 38442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:28<00:00,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 36248\n",
      "sister 36248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 36248\n",
      "sister 36248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:35<00:00,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 34054\n",
      "sister 34054\n",
      "sister 34054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 34054\n",
      "sister 31860\n"
     ]
    }
   ],
   "source": [
    "shards = get_shards(sort_sisters(sisters_with_items), total_sister_items)\n",
    "#25%-75%\n",
    "for scope in np.linspace(0.25, 0.75, 3):\n",
    "    folds = get_sister_folds(shards, cleaned_full, scope)\n",
    "    for i in range(4):\n",
    "        save_sample(folds[i], 'sister', f'{DIRNAME}/no_sister_{int(scope*100)}_fold_{i+1}_seed_42.pickle')\n",
    "#100%\n",
    "save_sample(\n",
    "    get_sample(shards, cleaned_full, scope=1.0, i=0),\n",
    "    'sister',\n",
    "    f'{DIRNAME}/no_sister_100_seed_42.pickle'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676eda51-c900-41c1-a37e-4322f6435146",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-26T16:45:18.423251900Z",
     "iopub.status.idle": "2025-04-26T16:45:18.434330663Z",
     "shell.execute_reply": "2025-04-26T16:45:18.423222056Z"
    },
    "tags": []
   },
   "source": [
    "### Removing non-cohyponym items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16a3c3-cf34-4ab3-8a8b-7cfffd061120",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get distances to nearest test items for shard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9b5a1ac-e2de-4553-948a-003345e96c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:36:27.842640Z",
     "iopub.status.busy": "2025-04-27T00:36:27.841676Z",
     "iopub.status.idle": "2025-04-27T00:36:27.888153Z",
     "shell.execute_reply": "2025-04-27T00:36:27.887170Z",
     "shell.execute_reply.started": "2025-04-27T00:36:27.842583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wn_path(s1: str, s2: str) -> float:\n",
    "    '''\n",
    "    Get path between to items in the WordNet\n",
    "    \n",
    "    Arguments:\n",
    "        s1 -  name of a synset, e.g. 'synset1.n.01'\n",
    "        s2 - name of a synset, e.g. 'synset2.n.01'\n",
    "        \n",
    "    Returns: distance between two synsets\n",
    "    '''\n",
    "    return (1 / s1.path_similarity(s2)) - 1\n",
    "\n",
    "def get_distances(s1, s2_synsets) -> float:\n",
    "    '''\n",
    "    Calculate distance from synset s1 to the nearest synset from s2_synsets\n",
    "    '''\n",
    "    return min([get_wn_path(s1, s2) for s2 in s2_synsets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d920c783-b891-47f8-8499-f404539ae02b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:36:29.142303Z",
     "iopub.status.busy": "2025-04-27T00:36:29.141100Z",
     "iopub.status.idle": "2025-04-27T00:36:45.043651Z",
     "shell.execute_reply": "2025-04-27T00:36:45.042480Z",
     "shell.execute_reply.started": "2025-04-27T00:36:29.142254Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82115it [00:01, 63410.82it/s]\n",
      "13767it [00:00, 61020.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# test synsets\n",
    "SE_synsets = [wn.synset(s2) for s2 in create_cleaned_graph(**SE181A)[2]]\n",
    "# non-cohyponym synsets\n",
    "CLEANED_synsets = [\n",
    "    (\n",
    "        wn.synset(item['children']), wn.synset(item['parents'])\n",
    "    ) for item in cleaned_full if (item['children'], item['parents']) not in total_sister_items\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e332df-1e5d-4d84-8573-9bf3d0852d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import multiprocessing\n",
    "#from joblib import Parallel, delayed\n",
    "\n",
    "#N_JOBS = multiprocessing.cpu_count() - 1\n",
    "#calculate distances to children synsets\n",
    "#min_distances = Parallel(n_jobs=N_JOBS)(delayed(get_distances)(s1[0]) for s1 in tqdm(CLEANED_synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1404901-0907-4858-acbc-34868ad63b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:37:25.574785Z",
     "iopub.status.busy": "2025-04-27T00:37:25.573568Z",
     "iopub.status.idle": "2025-04-27T00:37:25.599687Z",
     "shell.execute_reply": "2025-04-27T00:37:25.598845Z",
     "shell.execute_reply.started": "2025-04-27T00:37:25.574732Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load calculated distances\n",
    "with open('wn_min_distances.txt', 'r') as fin:\n",
    "    distances = [int(x.strip()) for x in fin.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b06c5f19-69f1-4df8-9c2c-3634251e6540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:37:27.496286Z",
     "iopub.status.busy": "2025-04-27T00:37:27.494974Z",
     "iopub.status.idle": "2025-04-27T00:37:27.678856Z",
     "shell.execute_reply": "2025-04-27T00:37:27.677868Z",
     "shell.execute_reply.started": "2025-04-27T00:37:27.496215Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#train kmeans to define clusters of distances\n",
    "min_distances = np.array(distances).reshape(-1, 1)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "kmeans.fit(min_distances)\n",
    "clusters = kmeans.predict(min_distances)\n",
    "\n",
    "partitions = [[], [], []]\n",
    "for number, cluster in zip(min_distances.flatten(), clusters):\n",
    "    partitions[cluster].append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7e280ae-5358-4942-9a33-cd5f2584e07d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:37:32.863279Z",
     "iopub.status.busy": "2025-04-27T00:37:32.861994Z",
     "iopub.status.idle": "2025-04-27T00:37:32.892306Z",
     "shell.execute_reply": "2025-04-27T00:37:32.891268Z",
     "shell.execute_reply.started": "2025-04-27T00:37:32.863223Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2, 15]\n",
      "[3, 1, 6]\n",
      "[18195, 9801, 3864]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(lambda x: max(x), partitions)))\n",
    "print(list(map(lambda x: min(x), partitions)))\n",
    "print(list(map(lambda x: len(x), partitions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b107e2-5cab-4a0f-81ef-5fec877588e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:37:34.851620Z",
     "iopub.status.busy": "2025-04-27T00:37:34.850302Z",
     "iopub.status.idle": "2025-04-27T00:37:47.803701Z",
     "shell.execute_reply": "2025-04-27T00:37:47.802742Z",
     "shell.execute_reply.started": "2025-04-27T00:37:34.851569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40636/40636 [00:12<00:00, 3149.54it/s]\n",
      "31860it [00:00, 1113327.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# items without cohyponym items\n",
    "cleaned_other = [item for item in tqdm(cleaned_full) if (item['children'], item['parents']) not in total_sister_items]\n",
    "# items with assigned cluster\n",
    "total_other = {(item['children'], item['parents']): cl for item, cl in tqdm(zip(cleaned_other, clusters))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e7e8a82-ace3-408a-93a4-68fe0fd9370d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:38:06.353797Z",
     "iopub.status.busy": "2025-04-27T00:38:06.352583Z",
     "iopub.status.idle": "2025-04-27T00:39:33.828545Z",
     "shell.execute_reply": "2025-04-27T00:39:33.827651Z",
     "shell.execute_reply.started": "2025-04-27T00:38:06.353743Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31860/31860 [00:00<00:00, 2174020.62it/s]\n",
      "100%|██████████| 39209/39209 [01:27<00:00, 448.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# all non-cohyponym synset names\n",
    "all_other = list(set([x for item in tqdm(total_other) for x in item]))\n",
    "# synset names with associated items\n",
    "node_to_items = {\n",
    "    elem: [item for item in total_other if elem in item]\n",
    "    for elem in tqdm(all_other)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8fe1dba-a0f5-4149-97a0-b4d86c1fe1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:33.831318Z",
     "iopub.status.busy": "2025-04-27T00:39:33.830391Z",
     "iopub.status.idle": "2025-04-27T00:39:33.848676Z",
     "shell.execute_reply": "2025-04-27T00:39:33.847713Z",
     "shell.execute_reply.started": "2025-04-27T00:39:33.831283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def item_label(labels: list) -> int:\n",
    "    '''\n",
    "    Get label for synset based on paths from associated items to nearest test nodes\n",
    "    '''\n",
    "    if 2 in labels:\n",
    "        return 2\n",
    "    if 0 in labels:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "232444a2-6973-4a6d-aab3-ebab1b539fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:33.851441Z",
     "iopub.status.busy": "2025-04-27T00:39:33.850483Z",
     "iopub.status.idle": "2025-04-27T00:39:34.052211Z",
     "shell.execute_reply": "2025-04-27T00:39:34.051221Z",
     "shell.execute_reply.started": "2025-04-27T00:39:33.851401Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39209/39209 [00:00<00:00, 211310.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#assign label to each non-cohyponym synset depending on its associated item labels \n",
    "paths = {k:item_label([total_other[pair] for pair in v]) for k, v in tqdm(node_to_items.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ce007b1-8350-455d-969d-6e1ca27cb661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:34.055863Z",
     "iopub.status.busy": "2025-04-27T00:39:34.054107Z",
     "iopub.status.idle": "2025-04-27T00:39:34.091787Z",
     "shell.execute_reply": "2025-04-27T00:39:34.090786Z",
     "shell.execute_reply.started": "2025-04-27T00:39:34.055819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_by_clusters(paths: dict, node_to_items: dict) -> list[list]:\n",
    "    '''\n",
    "    Divide items by clusters (based on distances to test items) and shuffle\n",
    "    \n",
    "    Arguments:\n",
    "       paths - synset-label pairs\n",
    "       node_to_items - synset-items pairs\n",
    "       \n",
    "    Returns: list of items grouped by cluster\n",
    "    '''\n",
    "    clustered = []\n",
    "    for i in range(3):\n",
    "        clustered.append(list({k:node_to_items[k] for k, v in paths.items() if v == i}.items()))\n",
    "        random.seed(42)\n",
    "        random.shuffle(clustered[i])\n",
    "    return clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23ef5f4b-06a5-4ab0-bd02-3203368589ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:34.095261Z",
     "iopub.status.busy": "2025-04-27T00:39:34.093556Z",
     "iopub.status.idle": "2025-04-27T00:39:34.190039Z",
     "shell.execute_reply": "2025-04-27T00:39:34.189040Z",
     "shell.execute_reply.started": "2025-04-27T00:39:34.095218Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered = group_by_clusters(paths, node_to_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b3b90b6-e6ad-43ca-bd5f-d5973f14732d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:34.192792Z",
     "iopub.status.busy": "2025-04-27T00:39:34.192045Z",
     "iopub.status.idle": "2025-04-27T00:39:34.245744Z",
     "shell.execute_reply": "2025-04-27T00:39:34.244805Z",
     "shell.execute_reply.started": "2025-04-27T00:39:34.192746Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22688, 9791, 6730]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(group) for group in clustered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71bc13-2171-4bbc-8ac6-73bfc96a619b",
   "metadata": {},
   "source": [
    "#### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ca5eae4-8756-45ba-a0ba-5948e963e6b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:34.248534Z",
     "iopub.status.busy": "2025-04-27T00:39:34.247135Z",
     "iopub.status.idle": "2025-04-27T00:39:34.269618Z",
     "shell.execute_reply": "2025-04-27T00:39:34.268607Z",
     "shell.execute_reply.started": "2025-04-27T00:39:34.248471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_other_shards(clustered_stack: list[list], partitions: list[list], total_other: list[str], max_length: int, seed=42) -> list[list]:\n",
    "    '''\n",
    "    Prepare shards for 4-fold cross-validation: normalize by path to nearest test node\n",
    "    \n",
    "    Arguments:\n",
    "        clustered_stack - non-cohyponym items grouped by path to nearest test node\n",
    "        total_other - list of all items associated with non-cohyponyms\n",
    "        max_length - size of shard (used for sister sample construction)\n",
    "        seed - random seed, optional\n",
    "        \n",
    "    Returns: list of shards\n",
    "    '''\n",
    "    total_n = len(total_other)\n",
    "    np.random.seed(seed)\n",
    "    def next_cluster(n=3): return np.random.choice(np.arange(0, n), p=[len(px) / total_n for px in partitions]) #4 fold cross-validation\n",
    "    shards = [[], [], [], []]\n",
    "    added = []\n",
    "    n_clusters = 3\n",
    "    while any([len(shard) < max_length for shard in shards]) and clustered_stack: #stack is not empty\n",
    "        for i in range(4):\n",
    "            if len(shards[i]) >= max_length:\n",
    "                continue\n",
    "            j = next_cluster(n_clusters)\n",
    "            _, v = clustered_stack[j].pop() \n",
    "            to_add = [item for item in v if item not in added]\n",
    "            added.extend(to_add)\n",
    "            shards[i].extend(to_add)\n",
    "            if not clustered_stack[j]:\n",
    "                total_n -= len(partitions[j])\n",
    "                partitions.pop(j)\n",
    "                clustered_stack.pop(j)\n",
    "                n_clusters -= 1\n",
    "            continue\n",
    "    return shards\n",
    "            \n",
    "\n",
    "def get_sample(shards: list[dict], dataset: list[dict], scope: float, i: int) -> list[dict]:\n",
    "    '''\n",
    "    Get sample for 4-fold cross-validation depending on proportion and index of fold\n",
    "    \n",
    "    Arguments:\n",
    "        shards - data partitions to use in sample construction\n",
    "        dataset - full training sample\n",
    "        scope - proportion of removed items\n",
    "        i - index of shard\n",
    "        \n",
    "    Returns: list of items dicts\n",
    "    '''\n",
    "    if scope == 0.25:\n",
    "        to_remove = shards[i]\n",
    "    if scope == 0.75:\n",
    "        to_remove = [pair for j in range(4) for pair in shards[j] if j != i]\n",
    "    if scope == 0.5:\n",
    "        indexes = list(combinations(range(4), 2))[i]\n",
    "        to_remove = shards[indexes[0]] + shards[indexes[1]]\n",
    "    if scope == 1.0:\n",
    "        to_remove = [item for shard in shards for item in shard]\n",
    "    #print(scope, i, to_remove, sep='\\n')\n",
    "    return [item for item in dataset if (item['children'], item['parents']) not in to_remove]\n",
    "        \n",
    "    \n",
    "def get_other_folds(shards: list[list], dataset: list[dict], scope: float) -> list[list]:\n",
    "    '''\n",
    "    Generate samples by removing shards from the training set following 4-fold cross-validation algorithm\n",
    "    \n",
    "    Arguments:\n",
    "        shards - shards to compose the subset for removal from\n",
    "        dataset - trainig sample\n",
    "        scope - proportion of items to remove\n",
    "        \n",
    "    Returns: list of concstructed samples\n",
    "    '''\n",
    "    k = 6 if scope == 0.5 else 4\n",
    "    return [get_sample(shards, dataset, scope, i) for i in tqdm(range(k))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c112a3c-316a-448a-b0fc-069ff5de9820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:34.272740Z",
     "iopub.status.busy": "2025-04-27T00:39:34.270970Z",
     "iopub.status.idle": "2025-04-27T00:39:36.301163Z",
     "shell.execute_reply": "2025-04-27T00:39:36.300100Z",
     "shell.execute_reply.started": "2025-04-27T00:39:34.272685Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate shards\n",
    "shards = get_other_shards(clustered, partitions, total_other, len(total_sister_items) // 4, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60b0b73a-8c0f-4acd-9400-097476139bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:36.304175Z",
     "iopub.status.busy": "2025-04-27T00:39:36.302407Z",
     "iopub.status.idle": "2025-04-27T00:39:36.343747Z",
     "shell.execute_reply": "2025-04-27T00:39:36.342766Z",
     "shell.execute_reply.started": "2025-04-27T00:39:36.304128Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_items([(0, 1219), (1, 793), (2, 182)]),\n",
       " dict_items([(2, 222), (0, 1243), (1, 729)]),\n",
       " dict_items([(1, 759), (0, 1245), (2, 190)]),\n",
       " dict_items([(1, 743), (2, 191), (0, 1260)])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "[Counter([total_other[item] for item in shard]).items() for shard in shards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6b3ef06-a7f9-4335-9488-d9d4dc148e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:39:36.347657Z",
     "iopub.status.busy": "2025-04-27T00:39:36.346466Z",
     "iopub.status.idle": "2025-04-27T00:39:36.371270Z",
     "shell.execute_reply": "2025-04-27T00:39:36.370314Z",
     "shell.execute_reply.started": "2025-04-27T00:39:36.347614Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2194, 2194, 2194, 2194]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(shard) for shard in shards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9261e282-756f-4839-a033-149f47b1e4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:41:26.035569Z",
     "iopub.status.busy": "2025-04-27T00:41:26.034150Z",
     "iopub.status.idle": "2025-04-27T00:42:54.558628Z",
     "shell.execute_reply": "2025-04-27T00:42:54.557682Z",
     "shell.execute_reply.started": "2025-04-27T00:41:26.035515Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl 38442\n",
      "compl 38442\n",
      "compl 38442\n",
      "compl 38442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:29<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl 36248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl 36248\n",
      "compl 36248\n",
      "compl 36248\n",
      "compl 36248\n",
      "compl 36248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:36<00:00,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl 34054\n",
      "compl 34054\n",
      "compl 34054\n",
      "compl 34054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl 31860\n"
     ]
    }
   ],
   "source": [
    "#25%-75%\n",
    "for scope in np.linspace(0.25, 0.75, 3):\n",
    "    folds = get_other_folds(shards, cleaned_full, scope)\n",
    "    k = 4 if scope != 0.5 else 6\n",
    "    for i in range(k):\n",
    "        save_sample(folds[i], 'compl', f'{DIRNAME}/no_compl_{int(scope*100)}_fold_{i+1}_seed_42.pickle')\n",
    "#100%\n",
    "save_sample(\n",
    "    get_sample(shards, cleaned_full, scope=1.0, i=0),\n",
    "    'compl',\n",
    "    f'{DIRNAME}/no_compl_100_seed_42.pickle'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b91cc-11a5-4a0f-a652-25426ffd0d47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Removing random items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a153c636-9799-461f-b550-320a2fa42dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:40:08.415554Z",
     "iopub.status.busy": "2025-04-26T23:40:08.414522Z",
     "iopub.status.idle": "2025-04-26T23:40:08.462820Z",
     "shell.execute_reply": "2025-04-26T23:40:08.461724Z",
     "shell.execute_reply.started": "2025-04-26T23:40:08.415497Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def next_node_type(nodes):\n",
    "    n_nodes = np.sum([len(node_type) for node_type in nodes])\n",
    "    return np.random.choice(np.arange(0, len(nodes)), p=[len(node_type) / n_nodes for node_type in nodes])\n",
    "\n",
    "def get_random_sample(dataset: list[dict], items: dict, nodes: list[list], length: int, scope: float, seed=42) -> list[dict]:\n",
    "    '''\n",
    "    Get samples with randomly deleted nodes \n",
    "    \n",
    "    Arguments:\n",
    "        dataset - trainig sample\n",
    "        items - dict of synset with associated items pairs\n",
    "        nodes - list of list with nodes by type\n",
    "        length - size of shard (used for sister sample construction)\n",
    "        scope - proportion of items to remove\n",
    "        \n",
    "    Returns: constructed sample\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    to_remove = []\n",
    "    while len(to_remove) < length * scope:\n",
    "        node_type = next_node_type(nodes)\n",
    "        index = random.randint(0, len(nodes[node_type]) - 1)\n",
    "        candidates = [item for item in items[nodes[node_type][index]] if item not in to_remove]\n",
    "        to_remove.extend(candidates)\n",
    "        nodes[node_type].pop(index)\n",
    "        \n",
    "    return [item for item in dataset if (item['children'], item['parents']) not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2814931c-a789-4336-b306-eb146d083c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:36:12.065580Z",
     "iopub.status.busy": "2025-04-26T23:36:12.064414Z",
     "iopub.status.idle": "2025-04-26T23:39:54.130389Z",
     "shell.execute_reply": "2025-04-26T23:39:54.129340Z",
     "shell.execute_reply.started": "2025-04-26T23:36:12.065540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40636/40636 [00:00<00:00, 1110646.01it/s]\n",
      "100%|██████████| 40636/40636 [00:00<00:00, 2047545.53it/s]\n",
      "100%|██████████| 47710/47710 [03:41<00:00, 214.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#collect all hyponym-hypernym pairs as tuples\n",
    "all_items = [(item['children'], item['parents']) for item in tqdm(cleaned_full)]\n",
    "#collect all synsets\n",
    "all_synsets = list(set([x for item in tqdm(all_items) for x in item]))\n",
    "#synset names with associated items\n",
    "node_to_items = {\n",
    "    elem: [item for item in all_items if elem in item]\n",
    "    for elem in tqdm(all_synsets)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6f6a5a18-b235-49cb-b96c-45cadcd377f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:39:56.250900Z",
     "iopub.status.busy": "2025-04-26T23:39:56.249201Z",
     "iopub.status.idle": "2025-04-26T23:39:56.268822Z",
     "shell.execute_reply": "2025-04-26T23:39:56.267831Z",
     "shell.execute_reply.started": "2025-04-26T23:39:56.250843Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = len(total_sister_items)\n",
    "SEEDS = [42, 13, 99]\n",
    "nodes = [all_other, rest_sisters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "828ec43f-5d80-4bdf-bd52-ca5590fb697d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:40:14.108940Z",
     "iopub.status.busy": "2025-04-26T23:40:14.107655Z",
     "iopub.status.idle": "2025-04-26T23:42:38.426870Z",
     "shell.execute_reply": "2025-04-26T23:42:38.425824Z",
     "shell.execute_reply.started": "2025-04-26T23:40:14.108894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 38442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:11<00:12,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 36248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:27<00:10, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 34053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 31860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 38442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:12<00:13,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 36247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:27<00:10, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 34053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:47<00:00, 11.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 31859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 38442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:09,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 36248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:27<00:10, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 34053\n",
      "random 31860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for seed in SEEDS:\n",
    "    for scope in tqdm(np.linspace(0.25, 1.0, 4)):\n",
    "        save_sample(\n",
    "            get_random_sample(cleaned_full, node_to_items, deepcopy(nodes), max_length, scope, seed=seed),\n",
    "            'random', f'{DIRNAME}/no_rand_{int(scope*100)}_seed_{seed}.pickle'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65c866-151d-4abd-8474-62a14603dc8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Constructing samples with definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "63873120-a5c5-46dc-a50e-50559d04e075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:59:07.529339Z",
     "iopub.status.busy": "2025-04-26T23:59:07.528233Z",
     "iopub.status.idle": "2025-04-26T23:59:07.548598Z",
     "shell.execute_reply": "2025-04-26T23:59:07.547655Z",
     "shell.execute_reply.started": "2025-04-26T23:59:07.529289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_definitions(elem: dict):\n",
    "    '''\n",
    "    Add definition to dataset item depending on condition\n",
    "    \n",
    "    Arguments:\n",
    "        elem - item of dataset\n",
    "    '''\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "\n",
    "def definitions(train: list[dict], subset=None, inverse=False) -> list[dict]:\n",
    "    '''\n",
    "    Add definitions to items of train or subset depending on condition (inverse)\n",
    "        train - training sample\n",
    "        subset - subset to check for current element containment\n",
    "        inverse - if False, assign definition to elements from the subset only, else - to all except elements in the subset \n",
    "        \n",
    "    Returns: train\n",
    "    '''\n",
    "    added_def_counter = 0\n",
    "    for i, elem in enumerate(train):\n",
    "        try:\n",
    "            if inverse:\n",
    "                if not subset or elem['children'] not in subset:\n",
    "                    add_definitions(elem)\n",
    "                    added_def_counter += 1\n",
    "                    continue\n",
    "                elem['child_def'] = ''  \n",
    "                continue\n",
    "            if not subset or elem['children'] in subset:\n",
    "                add_definitions(elem)\n",
    "                added_def_counter += 1\n",
    "                continue\n",
    "            elem['child_def'] = ''              \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            print(i, elem)\n",
    "            train.remove(elem)\n",
    "\n",
    "    #print(counter)\n",
    "    print('Added', added_def_counter)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e3f6565f-705a-406d-b859-2cd1ecaab9cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:00:15.384394Z",
     "iopub.status.busy": "2025-04-27T00:00:15.382940Z",
     "iopub.status.idle": "2025-04-27T00:00:15.462010Z",
     "shell.execute_reply": "2025-04-27T00:00:15.460859Z",
     "shell.execute_reply.started": "2025-04-27T00:00:15.384332Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_def_subset(train: list[dict], sisters: list[str], mode: str, treshold: float, seed=42) -> list[str]:\n",
    "    '''\n",
    "    Collect subset to manage assignment of definition\n",
    "    \n",
    "    Arguments:\n",
    "        train - training sample\n",
    "        sisters- list of sister terms\n",
    "        mode - experimental condition\n",
    "        threshold - proportion of elements to include in the susbset\n",
    "        seed - random seed, optional\n",
    "        \n",
    "    Returns: subset (list of synset names)\n",
    "    '''\n",
    "    train = [elem['children'] for elem in train]\n",
    "    random.seed(seed)\n",
    "    if mode == 'sister':\n",
    "        return sample(sisters, treshold)\n",
    "    if mode == 'compl':\n",
    "        return sample([x for x in train if x not in sisters], treshold)\n",
    "    if mode == 'rand':\n",
    "        return sample(train, treshold)\n",
    "    \n",
    "def definition_samples(dirname: str, seed: int, modes=('sister', 'compl', 'rand'), scopes=np.linspace(0, 1, 5), **kwargs) -> None:\n",
    "    '''\n",
    "    Generate samples with definitions\n",
    "        seed - random seed\n",
    "        modes - experimental conditions\n",
    "        scopes - proportions of items to add definitions to\n",
    "        kwargs - kwargs to pass to the WordNet graph builder\n",
    "    '''\n",
    "    G, test_parents, _ = create_cleaned_graph(**kwargs)\n",
    "    sisters = get_sisters(G, test_parents)\n",
    "    train = get_train(G, with_test=False)\n",
    "    rest_sisters = get_fraction(train, sisters, out_list=True)\n",
    "    \n",
    "    for scope in tqdm(scopes):\n",
    "        if scope == 0.0:\n",
    "            train_def = definitions(train)\n",
    "            train_out = f'{dirname}/full_train_def_train.pickle'\n",
    "            save_sample(train_def, 'full', train_out)\n",
    "            continue\n",
    "        treshold = int(len(rest_sisters) * scope)\n",
    "        for mode in modes:\n",
    "            subset = get_def_subset(train, rest_sisters, mode, treshold, seed=seed)\n",
    "            print(mode, int(scope*100), len(subset))\n",
    "            for inverse in (True, False):\n",
    "                print(f'inverse={inverse}')\n",
    "                train_def = definitions(train, subset=subset, inverse=inverse)\n",
    "                inv = '_inv' if inverse else ''\n",
    "                train_out = f'{dirname}/def{inv}_{mode}_{str(int(scope*100))}_seed_{seed}_train.pickle'\n",
    "                save_sample(train_def, mode, train_out)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "55da0da1-2e58-4352-92d0-36acb6640220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:00:17.219087Z",
     "iopub.status.busy": "2025-04-27T00:00:17.217646Z",
     "iopub.status.idle": "2025-04-27T00:02:17.697991Z",
     "shell.execute_reply": "2025-04-27T00:02:17.697014Z",
     "shell.execute_reply.started": "2025-04-27T00:00:17.219029Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82115it [00:01, 65834.81it/s]\n",
      "13767it [00:00, 61924.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 40636 40636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 40636\n",
      "full 40636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:01,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister 25 1383\n",
      "inverse=True\n",
      "Added 39203\n",
      "sister 40636\n",
      "\n",
      "inverse=False\n",
      "Added 1433\n",
      "sister 40636\n",
      "\n",
      "compl 25 1383\n",
      "inverse=True\n",
      "Added 39195\n",
      "compl 40636\n",
      "\n",
      "inverse=False\n",
      "Added 1441\n",
      "compl 40636\n",
      "\n",
      "rand 25 1383\n",
      "inverse=True\n",
      "Added 39193\n",
      "rand 40636\n",
      "\n",
      "inverse=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:12<00:21,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1443\n",
      "rand 40636\n",
      "\n",
      "sister 50 2767\n",
      "inverse=True\n",
      "Added 37760\n",
      "sister 40636\n",
      "\n",
      "inverse=False\n",
      "Added 2876\n",
      "sister 40636\n",
      "\n",
      "compl 50 2767\n",
      "inverse=True\n",
      "Added 37759\n",
      "compl 40636\n",
      "\n",
      "inverse=False\n",
      "Added 2877\n",
      "compl 40636\n",
      "\n",
      "rand 50 2767\n",
      "inverse=True\n",
      "Added 37761\n",
      "rand 40636\n",
      "\n",
      "inverse=False\n",
      "Added 2875\n",
      "rand 40636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:31<00:25, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sister 75 4151\n",
      "inverse=True\n",
      "Added 36328\n",
      "sister 40636\n",
      "\n",
      "inverse=False\n",
      "Added 4308\n",
      "sister 40636\n",
      "\n",
      "compl 75 4151\n",
      "inverse=True\n",
      "Added 36329\n",
      "compl 40636\n",
      "\n",
      "inverse=False\n",
      "Added 4307\n",
      "compl 40636\n",
      "\n",
      "rand 75 4151\n",
      "inverse=True\n",
      "Added 36313\n",
      "rand 40636\n",
      "\n",
      "inverse=False\n",
      "Added 4323\n",
      "rand 40636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:59<00:18, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sister 100 5535\n",
      "inverse=True\n",
      "Added 34891\n",
      "sister 40636\n",
      "\n",
      "inverse=False\n",
      "Added 5745\n",
      "sister 40636\n",
      "\n",
      "compl 100 5535\n",
      "inverse=True\n",
      "Added 34925\n",
      "compl 40636\n",
      "\n",
      "inverse=False\n",
      "Added 5711\n",
      "compl 40636\n",
      "\n",
      "rand 100 5535\n",
      "inverse=True\n",
      "Added 34885\n",
      "rand 40636\n",
      "\n",
      "inverse=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:38<00:00, 26.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5751\n",
      "rand 40636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:38<00:00, 19.72s/it]\n"
     ]
    }
   ],
   "source": [
    "definition_samples(dirname=DIRNAME, seed=42, **SE181A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895b607-f6cf-427d-b486-fe14d8f4c625",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Analysis of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21ca2f20-9d2c-4962-b9e4-3aae5ee2bf0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:46:13.916848Z",
     "iopub.status.busy": "2025-04-27T00:46:13.915532Z",
     "iopub.status.idle": "2025-04-27T00:46:13.942323Z",
     "shell.execute_reply": "2025-04-27T00:46:13.941367Z",
     "shell.execute_reply.started": "2025-04-27T00:46:13.916796Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path: str) -> list[dict]:\n",
    "    '''\n",
    "    Unpickle training sample\n",
    "    '''\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def get_subset_of_sisters(dirname: str, base_sample_name: str, **kwargs) -> list[str]:\n",
    "    '''\n",
    "    Get subset of sisters from the base sample\n",
    "    \n",
    "    Arguments\n",
    "        dirname - name of the directory\n",
    "        base_sample_name - name of pickle file with base sample\n",
    "        kwargs - keyword arguments to pass to the create_cleaned_graph function\n",
    "    '''\n",
    "    G, test_parents, _ = create_cleaned_graph(**kwargs)\n",
    "    sisters = get_sisters(G, test_parents)\n",
    "    base = read_file(os.path.join(dirname, base_sample_name))\n",
    "    return get_fraction(base, sisters, out_list=True)\n",
    "\n",
    "def fraction_of_removed_items(dirname: str, prefix: str, subset: list[str]) -> None:\n",
    "    '''\n",
    "    Print proportion of removed items from subset in the training sets from directory\n",
    "    \n",
    "    Arguments\n",
    "        dirname - name of the directory\n",
    "        prefix - prefix of filenames with training sets\n",
    "        subset - list of items to check for in each training set\n",
    "    '''\n",
    "\n",
    "    for fname in sorted(os.listdir(dirname)):\n",
    "        if prefix in fname[:len(prefix)]:\n",
    "            train_set = read_file(os.path.join(dirname, fname))\n",
    "            fraq_sisters = get_fraction(train_set, subset)\n",
    "            print(f'{fname[:fname.index(\".\")]}:\\t\\t{fraq_sisters:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28ae0f09-1b11-4c77-a9b9-3b9a8df08ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:46:16.888538Z",
     "iopub.status.busy": "2025-04-27T00:46:16.887462Z",
     "iopub.status.idle": "2025-04-27T00:48:16.920569Z",
     "shell.execute_reply": "2025-04-27T00:48:16.919610Z",
     "shell.execute_reply.started": "2025-04-27T00:46:16.888486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82115it [00:01, 64098.63it/s]\n",
      "13767it [00:00, 61034.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_compl_100_seed_42:\t\t0.000\n",
      "no_compl_25_fold_1_seed_42:\t\t0.000\n",
      "no_compl_25_fold_2_seed_42:\t\t0.000\n",
      "no_compl_25_fold_3_seed_42:\t\t0.000\n",
      "no_compl_25_fold_4_seed_42:\t\t0.000\n",
      "no_compl_50_fold_1_seed_42:\t\t0.000\n",
      "no_compl_50_fold_2_seed_42:\t\t0.000\n",
      "no_compl_50_fold_3_seed_42:\t\t0.000\n",
      "no_compl_50_fold_4_seed_42:\t\t0.000\n",
      "no_compl_50_fold_5_seed_42:\t\t0.000\n",
      "no_compl_50_fold_6_seed_42:\t\t0.000\n",
      "no_compl_75_fold_1_seed_42:\t\t0.000\n",
      "no_compl_75_fold_2_seed_42:\t\t0.000\n",
      "no_compl_75_fold_3_seed_42:\t\t0.000\n",
      "no_compl_75_fold_4_seed_42:\t\t0.000\n",
      "no_rand_100_seed_13:\t\t0.151\n",
      "no_rand_100_seed_42:\t\t0.155\n",
      "no_rand_100_seed_99:\t\t0.154\n",
      "no_rand_25_seed_13:\t\t0.041\n",
      "no_rand_25_seed_42:\t\t0.041\n",
      "no_rand_25_seed_99:\t\t0.041\n",
      "no_rand_50_seed_13:\t\t0.075\n",
      "no_rand_50_seed_42:\t\t0.075\n",
      "no_rand_50_seed_99:\t\t0.085\n",
      "no_rand_75_seed_13:\t\t0.117\n",
      "no_rand_75_seed_42:\t\t0.112\n",
      "no_rand_75_seed_99:\t\t0.118\n",
      "no_sister_100_seed_42:\t\t1.000\n",
      "no_sister_25_fold_1_seed_42:\t\t0.254\n",
      "no_sister_25_fold_2_seed_42:\t\t0.250\n",
      "no_sister_25_fold_3_seed_42:\t\t0.250\n",
      "no_sister_25_fold_4_seed_42:\t\t0.241\n",
      "no_sister_50_fold_1_seed_42:\t\t0.504\n",
      "no_sister_50_fold_2_seed_42:\t\t0.504\n",
      "no_sister_50_fold_3_seed_42:\t\t0.495\n",
      "no_sister_50_fold_4_seed_42:\t\t0.501\n",
      "no_sister_75_fold_1_seed_42:\t\t0.744\n",
      "no_sister_75_fold_2_seed_42:\t\t0.747\n",
      "no_sister_75_fold_3_seed_42:\t\t0.747\n",
      "no_sister_75_fold_4_seed_42:\t\t0.756\n"
     ]
    }
   ],
   "source": [
    "fraction_of_removed_items(DIRNAME, 'no_', subset=get_subset_of_sisters(DIRNAME, 'clean_train.pickle', **SE181A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e7aeeaef-1b09-4448-9432-b7a564891eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:08:58.441308Z",
     "iopub.status.busy": "2025-04-27T00:08:58.439558Z",
     "iopub.status.idle": "2025-04-27T00:08:58.453976Z",
     "shell.execute_reply": "2025-04-27T00:08:58.453131Z",
     "shell.execute_reply.started": "2025-04-27T00:08:58.441246Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fraction_of_items_definitions(dirname: str, prefix: str, subset: list[str]) -> None:\n",
    "    '''\n",
    "    Print proportion of items with definitions from subset in the training sets from directory\n",
    "    \n",
    "    Arguments\n",
    "        dirname - name of the directory\n",
    "        prefix - prefix of filenames with training sets\n",
    "        subset - list of synsets to check for in each training set\n",
    "    '''\n",
    "    for fname in sorted(os.listdir(dirname)):\n",
    "        if prefix in fname[:len(prefix)]:\n",
    "            train_set = read_file(os.path.join(dirname, fname))\n",
    "            fraq_type = len([item['children'] for item in train_set if item['child_def']]) / len(subset)\n",
    "            fraq_sisters = len(set([item['children'] for item in train_set if item['child_def'] and item['children'] in subset])) / len(subset)\n",
    "            fraq_total = len([item['children'] for item in train_set if item['child_def']]) / len(train_set)\n",
    "            print(f'{fname[:fname.index(\".\")]}:\\t\\t{fraq_total*100:.3f} (total)\\t{fraq_type*100:.2f} (def)\\t{fraq_sisters*100:.2f} (sisters)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "68b0e820-2be9-4734-b185-5f74d7c1420f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T00:06:32.358471Z",
     "iopub.status.busy": "2025-04-27T00:06:32.357135Z",
     "iopub.status.idle": "2025-04-27T00:07:33.675139Z",
     "shell.execute_reply": "2025-04-27T00:07:33.673918Z",
     "shell.execute_reply.started": "2025-04-27T00:06:32.358418Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82115it [00:01, 62905.20it/s]\n",
      "13767it [00:00, 61379.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def_compl_100_seed_42_train:\t\t14.054 (total)\t99.60 (def)\t0.00 (sisters)\n",
      "def_compl_25_seed_42_train:\t\t3.546 (total)\t24.97 (def)\t0.00 (sisters)\n",
      "def_compl_50_seed_42_train:\t\t7.080 (total)\t49.97 (def)\t0.00 (sisters)\n",
      "def_compl_75_seed_42_train:\t\t10.599 (total)\t74.87 (def)\t0.00 (sisters)\n",
      "def_inv_compl_100_seed_42_train:\t\t85.946 (total)\t619.31 (def)\t100.00 (sisters)\n",
      "def_inv_compl_25_seed_42_train:\t\t96.454 (total)\t693.95 (def)\t100.00 (sisters)\n",
      "def_inv_compl_50_seed_42_train:\t\t92.920 (total)\t668.94 (def)\t100.00 (sisters)\n",
      "def_inv_compl_75_seed_42_train:\t\t89.401 (total)\t644.05 (def)\t100.00 (sisters)\n",
      "def_inv_rand_100_seed_42_train:\t\t85.848 (total)\t619.22 (def)\t86.32 (sisters)\n",
      "def_inv_rand_25_seed_42_train:\t\t96.449 (total)\t693.97 (def)\t96.42 (sisters)\n",
      "def_inv_rand_50_seed_42_train:\t\t92.925 (total)\t669.03 (def)\t93.03 (sisters)\n",
      "def_inv_rand_75_seed_42_train:\t\t89.362 (total)\t644.08 (def)\t89.83 (sisters)\n",
      "def_inv_sister_100_seed_42_train:\t\t85.862 (total)\t618.92 (def)\t0.00 (sisters)\n",
      "def_inv_sister_25_seed_42_train:\t\t96.474 (total)\t693.93 (def)\t75.01 (sisters)\n",
      "def_inv_sister_50_seed_42_train:\t\t92.923 (total)\t668.93 (def)\t50.01 (sisters)\n",
      "def_inv_sister_75_seed_42_train:\t\t89.399 (total)\t643.92 (def)\t25.00 (sisters)\n",
      "def_rand_100_seed_42_train:\t\t14.152 (total)\t99.69 (def)\t13.68 (sisters)\n",
      "def_rand_25_seed_42_train:\t\t3.551 (total)\t24.95 (def)\t3.58 (sisters)\n",
      "def_rand_50_seed_42_train:\t\t7.075 (total)\t49.88 (def)\t6.97 (sisters)\n",
      "def_rand_75_seed_42_train:\t\t10.638 (total)\t74.83 (def)\t10.17 (sisters)\n",
      "def_sister_100_seed_42_train:\t\t14.138 (total)\t100.00 (def)\t100.00 (sisters)\n",
      "def_sister_25_seed_42_train:\t\t3.526 (total)\t24.99 (def)\t24.99 (sisters)\n",
      "def_sister_50_seed_42_train:\t\t7.077 (total)\t49.99 (def)\t49.99 (sisters)\n",
      "def_sister_75_seed_42_train:\t\t10.601 (total)\t75.00 (def)\t75.00 (sisters)\n"
     ]
    }
   ],
   "source": [
    "fraction_of_items_definitions(DIRNAME, 'def_', get_subset_of_sisters(DIRNAME, 'clean_train.pickle', **SE181A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
